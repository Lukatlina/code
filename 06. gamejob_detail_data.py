import pandas as pd
import os
import time
import logging
import re
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ê° ì„¹ì…˜ë³„ ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤ (ì´ì „ì— ìˆ˜ì •í•œ ìµœì¢… í•¨ìˆ˜ë“¤ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°) ---
def scrape_all_job_details(url, driver):
    """
    ëª¨ë“  ì»¬ëŸ¼ì˜ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    all_details = {}
    
    # ğŸ’¡ User-Agent ëª©ë¡
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0'
    ]

    try:
        # ğŸ’¡ ë§¤ë²ˆ ë‹¤ë¥¸ User-Agent ì„¤ì •
        random_user_agent = random.choice(user_agents)
        driver.execute_cdp_cmd('Network.setUserAgentOverride', {"userAgent": random_user_agent})
        
        # ğŸ’¡ ë¶ˆê·œì¹™í•œ ì‹œê°„ìœ¼ë¡œ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        random_wait_time = random.uniform(1, 3) # 2ì´ˆ ~ 5ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ê°’
        time.sleep(random_wait_time)

        driver.get(url)
        post_load_wait_time = random.uniform(1, 3) # 1ì´ˆ ~ 3ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ê°’
        time.sleep(post_load_wait_time)

        logger.info(f"í˜ì´ì§€ ì ‘ì† ì™„ë£Œ: {url}")


        # ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„: ëŒ€ê¸° ì‹œê°„ì„ 3~6ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ì„¤ì •
        try:
            WebDriverWait(driver, 5).until(
				        EC.presence_of_element_located((By.ID, 'gibOutline'))
				    )
            logger.info("âœ… 'ëª¨ì§‘ìš”ê°•' ì„¹ì…˜ ë¡œë”© í™•ì¸.")
        except TimeoutException:
            logger.warning("âš ï¸ 5ì´ˆ ë‚´ì— 'ëª¨ì§‘ìš”ê°•' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

				# ëª¨ì§‘ìš”ê°•, ë‹´ë‹¹ì—…ë¬´ ë° ìê²©ìš”ê±´, ìˆ˜ì •ì¼ ë° ë“±ë¡ì¼ í•¨ìˆ˜ ê°€ì ¸ì™€ì„œ ì €ì¥
        all_details['URL'] = url
        all_details.update(scrape_dates(driver))
        all_details.update(scrape_gib_outline(driver))
        all_details.update(scrape_job_duties_and_qualifications(driver))


        columns = [
            'URL', 'ëª¨ì§‘ë¶„ì•¼', 'í•´ë‹¹í‚¤ì›Œë“œ','ê²Œì„ë¶„ì•¼', 'ê³ ìš©í˜•íƒœ', 'ëª¨ì§‘ì¸ì›', 'ì±„ìš©ì§ê¸‰Â·ì§ì±…', 
            'ê¸‰ì—¬ì¡°ê±´', 'í•´ë‹¹ë¶„ì•¼', 'ìµœì¢…í•™ë ¥', 'ìê²©ì‚¬í•­', 'ì™¸êµ­ì–´ ëŠ¥ë ¥', 'ìê²©ì¦', 'ìˆ˜ì •ì¼', 'ë“±ë¡ì¼'
        ]

        for col in columns:
            if col not in all_details:
                all_details[col] = None

        return {'status': 'success', 'content': all_details}

    except WebDriverException as e:
        logger.error(f"âŒ WebDriver ì˜¤ë¥˜ ë°œìƒ (ìë™ ì¬ì‹œì‘): {e}")
        return {'status': 'webdriver_error', 'content': str(e)}
    except Exception as e:
        logger.error(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {'status': 'error', 'content': str(e)}

# --- ë“±ë¡ì¼, ìˆ˜ì •ì¼ ì¶”ì¶œ í•¨ìˆ˜ ---
def scrape_dates(driver):
    dates = {'ìˆ˜ì •ì¼': None, 'ë“±ë¡ì¼': None}

    # ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´: ì½œë¡ (:) ë’¤ì— ìˆëŠ” ë‚ ì§œì™€ ì‹œê°„ì„ ì¶”ì¶œ (ì˜ˆ: '2025-09-10 19:06')
    date_pattern = r':\s*(.*)'

    try:
        # ëª¨ë“  <p class="date"> ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        date_elements = driver.find_elements(By.CSS_SELECTOR, 'div#gibReadTop p.date')
        
        # ì²« ë²ˆì§¸ ìš”ì†Œ(ìˆ˜ì •ì¼)ì™€ ë‘ ë²ˆì§¸ ìš”ì†Œ(ë“±ë¡ì¼)ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        # find_elementsëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.
        if len(date_elements) > 0:
            modified_date_text = date_elements[0].text
            # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë‚ ì§œì™€ ì‹œê°„ ì¶”ì¶œ
            match = re.search(date_pattern, modified_date_text)
            if match:
                dates['ìˆ˜ì •ì¼'] = match.group(1).strip()
            # logger.info(f"âœ… ìˆ˜ì •ì¼: {dates['ìˆ˜ì •ì¼']} ì¶”ì¶œ ì™„ë£Œ.")
        
        if len(date_elements) > 1:
            registered_date_text = date_elements[1].text
            # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë‚ ì§œì™€ ì‹œê°„ ì¶”ì¶œ
            match = re.search(date_pattern, registered_date_text)
            if match:
                dates['ë“±ë¡ì¼'] = match.group(1).strip()
            # logger.info(f"âœ… ë“±ë¡ì¼: {dates['ë“±ë¡ì¼']} ì¶”ì¶œ ì™„ë£Œ.")

    except (NoSuchElementException, IndexError) as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    return dates




# --- ëª¨ì§‘ìš”ê°• ì¶”ì¶œ í•¨ìˆ˜ ---
def scrape_gib_outline(driver):
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ 'ëª¨ì§‘ìš”ê°•' ì„¹ì…˜ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        driver (WebDriver): Selenium WebDriver ê°ì²´.
    
    Returns:
        dict: ì¶”ì¶œëœ ëª¨ì§‘ìš”ê°• ì •ë³´.
    """
    gib_details = {
        'ëª¨ì§‘ë¶„ì•¼': None,
        'í•´ë‹¹í‚¤ì›Œë“œ': None,
        'ê²Œì„ë¶„ì•¼': None,
        'ê³ ìš©í˜•íƒœ': None,
        'ëª¨ì§‘ì¸ì›': None,
        'ì±„ìš©ì§ê¸‰Â·ì§ì±…': None,
        'ê¸‰ì—¬ì¡°ê±´': None,
        'í•´ë‹¹ë¶„ì•¼': None,  # ì§€ì›ìê²© í…Œì´ë¸” ë‚´
        'ì—°ë ¹': None,     # ì§€ì›ìê²© í…Œì´ë¸” ë‚´
        'ìµœì¢…í•™ë ¥': None,  # ì§€ì›ìê²© í…Œì´ë¸” ë‚´
        'ì„±ë³„': None,     # ì§€ì›ìê²© í…Œì´ë¸” ë‚´
        'ìê²©ì‚¬í•­': None,  # ìš°ëŒ€ì‚¬í•­ ë‚´
        'ì™¸êµ­ì–´ ëŠ¥ë ¥': None, # ìš°ëŒ€ì‚¬í•­ ë‚´
        'ìê²©ì¦': None,    # ìš°ëŒ€ì‚¬í•­ ë‚´
        'ì‚¬ì „ì¸í„°ë·°': None
    }
    try:
        # 'ëª¨ì§‘ìš”ê°•' ì„¹ì…˜ ì°¾ê¸°
        gib_outline_section = driver.find_element(By.ID, 'gibOutline')
        dl_tags = gib_outline_section.find_elements(By.TAG_NAME, 'dl')

        for dl_tag in dl_tags:
            dt_elements = dl_tag.find_elements(By.TAG_NAME, 'dt')
            for dt_element in dt_elements:
                key = dt_element.text.strip()
                
                # ë‹¤ìŒ í˜•ì œì¸ dd íƒœê·¸ ì°¾ê¸°
                dd_element = None
                try:
                    dd_element = dt_element.find_element(By.XPATH, 'following-sibling::dd[1]')
                except NoSuchElementException:
                    continue

                # 'ì§€ì›ìê²©' í…Œì´ë¸” ì²˜ë¦¬
                if key == 'ì§€ì›ìê²©':
                    try:
                        table = dd_element.find_element(By.TAG_NAME, 'table')
                        headers = [th.text.strip() for th in table.find_elements(By.TAG_NAME, 'th')]
                        values = [td.text.strip() for td in table.find_elements(By.TAG_NAME, 'td')]
                        for header, value in zip(headers, values):
                            gib_details[header] = value
                    except NoSuchElementException:
                        continue
                
                # 'ìš°ëŒ€ì‚¬í•­' ë‚´ì˜ ì„¸ë¶€ í•­ëª© ì²˜ë¦¬
                elif key == 'ìš°ëŒ€ì‚¬í•­':
                    try:
                        sub_dl = dd_element.find_element(By.TAG_NAME, 'dl')
                        sub_dt_elements = sub_dl.find_elements(By.TAG_NAME, 'dt')
                        for sub_dt in sub_dt_elements:
                            sub_key = sub_dt.text.strip()
                            sub_dd = sub_dt.find_element(By.XPATH, 'following-sibling::dd[1]')
                            gib_details[sub_key] = sub_dd.text.strip()
                    except NoSuchElementException:
                        # ìš°ëŒ€ì‚¬í•­ í•˜ìœ„ í•­ëª©ì´ ì—†ì„ ê²½ìš° íŒ¨ìŠ¤
                        continue
                
                # 'ê²Œì„ë¶„ì•¼' ì²˜ë¦¬
                elif key == 'ê²Œì„ë¶„ì•¼':
                    device_text = ""
                    genre_text = ""
                    try:
                        device_element = dd_element.find_element(By.CSS_SELECTOR, "font[color='#5e42a6']")
                        device_text = device_element.text.strip()
                    except NoSuchElementException:
                        pass
                    try:
                        genre_element = dd_element.find_element(By.CSS_SELECTOR, "font[color='#ae489e']")
                        genre_text = genre_element.text.strip()
                    except NoSuchElementException:
                        pass
                    gib_details['ê²Œì„ë¶„ì•¼'] = f"{device_text} {genre_text}".strip()

                # ê¸°íƒ€ ì¼ë°˜ í•­ëª© ì²˜ë¦¬
                elif key in gib_details:
                    # dd íƒœê·¸ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
                    full_text = dd_element.text.strip()
                    
                    # 'ëª¨ì§‘ì¸ì›'ì˜ ê²½ìš° ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                    if key == 'ëª¨ì§‘ì¸ì›':
                        # '0ëª… / í˜„ì¬ ì§€ì›ììˆ˜ : **ëª…' í˜•íƒœì—ì„œ '0ëª…'ë§Œ ê°€ì ¸ì˜´
                        if '/' in full_text:
                            full_text = full_text.split('/')[0].strip()

                    # ë§í¬ê°€ ìˆëŠ” ê²½ìš°, ë§í¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    # ì´ ì½”ë“œëŠ” ë§í¬ í…ìŠ¤íŠ¸ê°€ ë¶ˆí•„ìš”í•˜ê²Œ í¬í•¨ë˜ëŠ” ê²½ìš°ë¥¼ ì œê±°í•˜ê¸° ìœ„í•œ ë¡œì§ì…ë‹ˆë‹¤.
                    # 'ì±„ìš©ì§ê¸‰Â·ì§ì±…' ì²˜ëŸ¼ ì—¬ëŸ¬ ë§í¬ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ë‹¤ì‹œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
                    link_elements = dd_element.find_elements(By.TAG_NAME, 'a')
                    
                    if key == 'ëª¨ì§‘ë¶„ì•¼' or key == 'ì±„ìš©ì§ê¸‰Â·ì§ì±…':
                        if link_elements:
                            links_text = [link.text.strip() for link in link_elements]
                            gib_details[key] = ', '.join(links_text)
                        else:
                            gib_details[key] = full_text
                            
                    elif key == 'ê¸‰ì—¬ì¡°ê±´':
                        # 'ê¸‰ì—¬ì¡°ê±´'ì€ aíƒœê·¸ê°€ ìˆë“  ì—†ë“  ddì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
                        gib_details[key] = full_text
                    
                    else:
                        gib_details[key] = full_text

    
    except NoSuchElementException as e:
        print(f"âš ï¸ 'ëª¨ì§‘ìš”ê°•' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    return gib_details

# --- ëª¨ì§‘ìš”ê°• ì¶”ì¶œ í•¨ìˆ˜ ---
def scrape_iframe_content(driver, iframe_id):
    """
    ì§€ì •ëœ IDì˜ iframe ë‚´ë¶€ í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    Args:
        driver (WebDriver): Selenium WebDriver ê°ì²´.
        iframe_id (str): í¬ë¡¤ë§í•  iframeì˜ ID ('GI_Work_Content' ë˜ëŠ” 'GI_Comment').
    Returns:
        str: iframe ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸, ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ None.
    """
    try:
        # iframeì´ ë¡œë“œë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.ID, iframe_id))
        )
        iframe = driver.find_element(By.ID, iframe_id)
        
        # ë“œë¼ì´ë²„ì˜ í¬ì»¤ìŠ¤ë¥¼ iframeìœ¼ë¡œ ì „í™˜
        driver.switch_to.frame(iframe)
        
        # iframe ë‚´ë¶€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        content = driver.find_element(By.TAG_NAME, 'body').text
        
        # í¬ì»¤ìŠ¤ë¥¼ ë‹¤ì‹œ ë©”ì¸ í˜ì´ì§€ë¡œ ë³µê·€
        driver.switch_to.default_content()
        
        return content.strip()
        
    except (NoSuchElementException, TimeoutException) as e:
        print(f"âš ï¸ iframe '{iframe_id}'ë¥¼ ì°¾ê±°ë‚˜ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def scrape_job_duties_and_qualifications(driver):
    """
    ë‹´ë‹¹ì—…ë¬´ì™€ ìê²©ì¡°ê±´ì„ iframeì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    duties_and_qualifications = {
        'ë‹´ë‹¹ì—…ë¬´': None,
        'ìê²©ì¡°ê±´': None
    }
    
    # 'ë‹´ë‹¹ì—…ë¬´' iframe í¬ë¡¤ë§
    duties_and_qualifications['ë‹´ë‹¹ì—…ë¬´'] = scrape_iframe_content(driver, 'GI_Work_Content')
    
    # 'ìê²©ì¡°ê±´' iframe í¬ë¡¤ë§
    duties_and_qualifications['ìê²©ì¡°ê±´'] = scrape_iframe_content(driver, 'GI_Comment')
    
    return duties_and_qualifications



def create_dataframe_and_save(data, filename="job_details_test.csv"):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    logger.info(f"âœ… ë°ì´í„°ê°€ '{filename}' íŒŒì¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return df


# --- ìµœì¢… ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_path = 'C:/Users/nezumi/Documents/code/'
    input_file = 'gamejob.csv'
    full_path = os.path.join(file_path, input_file)
    
   
    df_existing = pd.read_csv(full_path)
    valid_urls_df = df_existing[df_existing['URL'].str.startswith('http', na=False)].copy()
   
    df_existing.dropna(subset=['URL'], inplace=True)

    # ğŸ’¡ ì¬ê°œ ì§€ì  ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!)
    # ì´ë¯¸ 130ê°œë¥¼ í¬ë¡¤ë§í–ˆë‹¤ë©´, 131ë²ˆì§¸ URLë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ 130ë²ˆì§¸ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
    start_index_to_resume = 0

    urls_to_crawl_df = valid_urls_df.iloc[start_index_to_resume:].copy()
    logger.info("âŒ ì²˜ìŒë¶€í„° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    total_urls = len(urls_to_crawl_df)

    if total_urls == 0:
        logger.info("âœ… ëª¨ë“  URLì„ ì´ë¯¸ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.")
        exit()

    chunk_size = 100 # ğŸ’¡ ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ 100ìœ¼ë¡œ ëŠ˜ë ¤ ì•ˆì •ì„± í–¥ìƒ
    total_chunks = (total_urls + chunk_size - 1) // chunk_size

    all_crawled_data = []


    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        # ë¶ˆí•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ ë¹„í™œì„±í™”
        chrome_options.add_argument("--disable-images")  # ì´ë¯¸ì§€ ì°¨ë‹¨
        chrome_options.add_argument("--disable-background-networking")  # ë°±ê·¸ë¼ìš´ë“œ ë„¤íŠ¸ì›Œí‚¹ ì°¨ë‹¨
        chrome_options.add_argument("--aggressive-cache-discard")  # ì ê·¹ì  ìºì‹œ ì •ë¦¬
        chrome_options.add_argument("--window-size=1920,1080")

        # ğŸ’¡ ì¶”ê°€ëœ ì½”ë“œ: ë¸Œë¼ìš°ì € ì§€ë¬¸ ìœ„ì¥
        # ë¸Œë¼ìš°ì €ê°€ ìë™í™”ë˜ê³  ìˆìŒì„ ì•Œë¦¬ëŠ” ë‚´ë¶€ í”Œë˜ê·¸ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        # ì›¹ ë“œë¼ì´ë²„ê°€ í™œì„±í™”ë  ë•Œ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” 'ìë™í™”(automation)' ìŠ¤ìœ„ì¹˜ë¥¼ ì œê±°
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        # 'ìë™í™” í™•ì¥ í”„ë¡œê·¸ë¨'ì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•˜ì—¬ ë´‡ ê°ì§€ë¥¼ íšŒí”¼
        chrome_options.add_experimental_option("useAutomationExtension", False)

        driver = webdriver.Chrome(options=chrome_options)
        
        for i in range(total_chunks):
            start_index = i * chunk_size
            end_index = start_index + chunk_size
            df_chunk = urls_to_crawl_df.iloc[start_index:end_index]
            
            logger.info(f"\n--- í˜„ì¬ {i+1}/{total_chunks}ë²ˆì§¸ ë¬¶ìŒ ì²˜ë¦¬ ì¤‘ (ì´ {len(df_chunk)}ê°œ) ---")

            # for idx, row in df_chunk.iterrows():
            for idx, row in df_chunk.iterrows():

                url = row['URL']
                
                if 'gamejob.co.kr' in url:
                    result = scrape_all_job_details(url, driver) 
                    
                    if result['status'] == 'success':
                        all_crawled_data.append(result['content'])
                        logger.info(f"[{idx+1}/{total_urls}] âœ… ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§ ì™„ë£Œ.")
                    else:
                        all_crawled_data.append({'URL': url, 'ì˜¤ë¥˜': result['content']})
                        logger.warning(f"[{idx+1}/{total_urls}] âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['content']}")
                        time.sleep(3) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ì‹œ ëŒ€ê¸°
                else:
                    all_crawled_data.append({'URL': url, 'ì˜¤ë¥˜': 'ë„ë©”ì¸ ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€'})
                    logger.warning(f"[{url}] âš ï¸ ë„ë©”ì¸ ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€.")
            
            # ì¤‘ê°„ ì €ì¥
            intermediate_df = pd.DataFrame(all_crawled_data)
            intermediate_df.to_csv(os.path.join(file_path, f'intermediate_crawled_data_retry_{i+1}.csv'), index=False, encoding='utf-8-sig')

            logger.info(f"âœ… {len(all_crawled_data)}ê°œ ë°ì´í„° ì¤‘ê°„ ì €ì¥ ì™„ë£Œ.")
            random_wait_time = random.uniform(1, 2)
            time.sleep(random_wait_time) # ë¬¶ìŒ ì²˜ë¦¬ í›„ ì¶”ê°€ ëŒ€ê¸°

        # driver.quit()
        
        # df_scraped_final = pd.DataFrame(all_crawled_data)
        # final_df = pd.merge(df_existing, df_scraped_final, left_on='URL', right_on='URL', how='left', suffixes=('', '_crawled'))
        
        # output_file = 'merged_job_postings_9218_final_final_(09_17).csv'
        # output_path = os.path.join(file_path, output_file)
        # final_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        logger.info("âœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ìµœì¢… ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    finally:
        if 'driver' in locals() and driver:
            driver.quit()