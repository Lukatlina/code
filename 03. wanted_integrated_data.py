import requests
import json
import time
import csv
import os
import random

# --- ë³€ìˆ˜ ì„¤ì • ---
BASE_URL_LIST = 'https://www.wanted.co.kr/api/chaos/navigation/v1/results'
BASE_URL_DETAIL = 'https://www.wanted.co.kr/api/chaos/jobs/v4'
ITEMS_PER_PAGE = 20

# ì¼ë°˜ì ì¸ ì›¹ ë¸Œë¼ìš°ì €ì˜ User-Agent í—¤ë”
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
}


all_jobs_data = []
offset = 0

print("ğŸš€ Wanted APIì—ì„œ ì „ì²´ ì±„ìš© ê³µê³  ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
print("====================================")

# --- 1ë‹¨ê³„: ê¸°ë³¸ ëª©ë¡ ë°ì´í„° ìˆ˜ì§‘ ---
while True:
    params = {
        int(time.time() * 1000): "",
        "job_group_id": 518,
        "country": "kr",
        "job_sort": "job.popularity_order",
        "years": -1,
        "locations": "all",
        "limit": ITEMS_PER_PAGE,
        "offset": offset,
    }
    
    try:
        response = requests.get(BASE_URL_LIST, params=params, headers=HEADERS)
        response.raise_for_status()
        data = response.json()
        items = data.get('data', [])

        if not items:
            print(f"âœ… offset {offset}ì—ì„œ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        for item in items:
            job_info = {
                'id': item.get('id', ''),
                'company_id': item.get('company', {}).get('id', ''),
                'company_name': item.get('company', {}).get('name', ''),
                'position': item.get('position', ''),
                'location': item.get('address', {}).get('location', ''),
                'district': item.get('address', {}).get('district', ''),
                'reward_total': item.get('reward_total', ''),
                'employment_type': item.get('employment_type', ''),
                'is_newbie': item.get('is_newbie', False),
                'annual_from': item.get('annual_from', None),
                'annual_to': item.get('annual_to', None),
                'skill_tags': item.get('skill_tags', []),
                'category_tag_id': item.get('category_tag', {}).get('id', ''),
                'user_oriented_tags': item.get('user_oriented_tags', [])
            }
            all_jobs_data.append(job_info)
            
        print(f"âœ”ï¸ í˜„ì¬ê¹Œì§€ {len(all_jobs_data)}ê°œì˜ ê¸°ë³¸ ê³µê³  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ.")
        offset += ITEMS_PER_PAGE
        time.sleep(random.uniform(0.5, 1.5))

    except requests.exceptions.RequestException as e:
        print(f"âŒ ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        break
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        break

print("\n--- 2ë‹¨ê³„: ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
# --- ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ë£¨í”„ ---
for job in all_jobs_data:
    job_id = job['id']
    detail_url = f"{BASE_URL_DETAIL}/{job_id}/details"
    
    try:
        response = requests.get(detail_url, headers=HEADERS)
        response.raise_for_status()
        detail_data = response.json().get('data', {}).get('job', {})
        
        if detail_data:
            detail_info = detail_data.get('detail', {})
            job['intro'] = detail_info.get('intro', '')
            job['main_tasks'] = detail_info.get('main_tasks', '')
            job['requirements'] = detail_info.get('requirements', '')
            job['preferred_points'] = detail_info.get('preferred_points', '')
            job['benefits'] = detail_info.get('benefits', '')
            job['hire_rounds'] = detail_info.get('hire_rounds', '')
            
            # ê¸°íƒ€ ì¶”ê°€ ì •ë³´
            job['full_location'] = detail_data.get('address', {}).get('full_location', '')
            job['category_tag_parent_id'] = detail_data.get('category_tag', {}).get('parent_tag', {}).get('id', '')
            job['category_tag_child_text'] = detail_data.get('category_tag', {}).get('child_tags', [{}])[0].get('text', '')

            # attraction_tagsì˜ titleì„ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
            attraction_tags = [tag.get('title', '') for tag in detail_data.get('attraction_tags', [])]
            job['attraction_tags'] = attraction_tags
            
            print(f"âœ”ï¸ ID {job_id} ìƒì„¸ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ.")
        else:
            print(f"âš ï¸ ID {job_id}ì˜ ìƒì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        time.sleep(random.uniform(0.5, 1.5))
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ID {job_id} ìƒì„¸ ë°ì´í„° ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
    except json.JSONDecodeError as e:
        print(f"âŒ ID {job_id} JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

print("====================================")
print(f"ì´ {len(all_jobs_data)}ê°œì˜ ê³µê³  ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

# --- ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ ---
file_path = 'wanted_full_job_data.csv'

if not all_jobs_data:
    print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    # CSV í—¤ë”(í•„ë“œëª…) ì •ì˜
    csv_header = list(all_jobs_data[0].keys())

    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ CSVì— ì €ì¥
    for item in all_jobs_data:
        for key in ['skill_tags', 'user_oriented_tags', 'attraction_tags']:
            if isinstance(item.get(key), list):
                item[key] = ', '.join([str(tag) for tag in item[key] if tag is not None])
            else:
                item[key] = ''

    try:
        with open(file_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=csv_header)
            writer.writeheader()
            writer.writerows(all_jobs_data)
        
        print(f"ë°ì´í„°ë¥¼ '{file_path}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        file_size = os.path.getsize(file_path)
        print(f"íŒŒì¼ í¬ê¸°: {file_size / 1024:.2f} KB")

    except IOError as e:
        print(f"CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")