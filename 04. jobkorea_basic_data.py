import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

from bs4 import BeautifulSoup
import re
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def scrape_all_job_listings_with_validation(url):
    """
    í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ëª¨ë“  ì±„ìš© ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ê³  í˜ì´ì§€ ì´ë™ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    job_listings = []
    driver = None
    
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.7339.81 Safari/537.36'
        chrome_options.add_argument(f'user-agent={user_agent}')

        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        logger.info(f"âœ… ì´ˆê¸° í˜ì´ì§€ ì ‘ì† ì™„ë£Œ: {url}")
        
        # í•„í„°ë§ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        # ... (AIÂ·ê°œë°œÂ·ë°ì´í„° ë²„íŠ¼ í´ë¦­, ì§ë¬´ ì„ íƒ, ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ë¡œì§)
        ai_dev_label = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, "//label[@for='duty_step1_10031']"))
        )
        ai_dev_label.click()
        logger.info("âœ… 'AIÂ·ê°œë°œÂ·ë°ì´í„°' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ.")
        time.sleep(2)

        sub_job_list_items = driver.find_elements(By.CSS_SELECTOR, '#duty_step2_10031_ly .item')

        if len(sub_job_list_items) == 0:
            logger.warning("âŒ í•˜ìœ„ ì§ë¬´ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i in range(min(12, len(sub_job_list_items))):
            label = sub_job_list_items[i].find_element(By.CSS_SELECTOR, '.lb_tag')
            label.click()
            job_name = sub_job_list_items[i].find_element(By.CSS_SELECTOR, 'input').get_attribute('data-name')
            logger.info(f"âœ… '{job_name}' ì§ë¬´ ì„ íƒ ì™„ë£Œ.")
        time.sleep(2)
        
        search_button = driver.find_element(By.ID, 'dev-btn-search')
        search_button.click()
        logger.info("âœ… ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ. ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")

        # ì…€ë ˆëŠ„ ë“œë¼ì´ë²„ 10ì´ˆì •ë„ idê°€ dvGIPagingì¸ ìš”ì†Œê°€ í˜ì´ì§€ì— ë‚˜íƒ€ë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼í•¨.
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'dvGIPaging')))
        time.sleep(2)
    
        
        page_num = 1
        while True:
            logger.info(f"\n--- {page_num}í˜ì´ì§€ ìŠ¤í¬ë© ì¤‘ ---")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, 'dvGIPaging'))
            )
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            table_body = soup.find('div', class_='tplList tplJobList')
            
            if not table_body:
                logger.warning("âš ï¸ ì±„ìš© ê³µê³  ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ë°ì´í„° ì¶”ì¶œ
            for tr in table_body.find_all('tr', class_='devloopArea'):
                try:
                    company_name = tr.find('td', class_='tplCo').a.text.strip()
                    job_title_element = tr.find('td', class_='tplTit').find('strong').find('a')
                    job_title = job_title_element.text.strip()
                    job_detail_url = f'https://www.jobkorea.co.kr{job_title_element["href"]}'

                    etc_info = tr.find('td', class_='tplTit').find('p', class_='etc').find_all('span', class_='cell')
                    experience = etc_info[0].text.strip() if len(etc_info) > 0 else None
                    education = etc_info[1].text.strip() if len(etc_info) > 1 else None
                    location = etc_info[2].text.strip() if len(etc_info) > 2 else None
                    job_type = etc_info[3].text.strip() if len(etc_info) > 3 else None
                    # ê¸‰ì—¬ì™€ ì§ê¸‰/ì§ì±… ë°ì´í„° ì¶”ì¶œ (ì¶”ê°€ëœ ë¶€ë¶„)
                    salary = etc_info[4].text.strip() if len(etc_info) > 4 else None
                    rank = etc_info[5].text.strip() if len(etc_info) > 5 else None

                    odd_td = tr.find('td', class_='odd')
                    registration_date = odd_td.find('span', class_='time').text.strip() if odd_td and odd_td.find('span', class_='time') else None
                    closing_date = odd_td.find('span', class_='date').text.strip() if odd_td and odd_td.find('span', class_='date') else None

                    job_listings.append({
                        'íšŒì‚¬ëª…': company_name,
                        'ì œëª©': job_title,
                        'ìƒì„¸í˜ì´ì§€_URL': job_detail_url,
                        'ê²½ë ¥': experience,
                        'í•™ë ¥': education,
                        'ì§€ì—­': location,
                        'ê³ ìš©í˜•íƒœ': job_type,
                        'ë“±ë¡ì¼': registration_date,
                        'ë§ˆê°ì¼': closing_date
                    })

                except Exception as e:
                    logger.warning(f"âš ï¸ ë°ì´í„° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•˜ëŠ” ë¡œì§
            next_page_num = page_num + 1
            try:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼(a íƒœê·¸)ì„ ì°¾ìŠµë‹ˆë‹¤.
                next_page_link_selector = f'div.tplPagination a[data-page="{next_page_num}"]'
                next_page_button = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, next_page_link_selector))
                )
                
                next_page_button.click()
                logger.info(f"âœ… {next_page_num}í˜ì´ì§€ ë²„íŠ¼ì„ í´ë¦­í–ˆìŠµë‹ˆë‹¤.")
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í–ˆëŠ”ì§€ ê²€ì¦
                now_page_selector = f'div.tplPagination span.now[data-page="{next_page_num}"]'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, now_page_selector))
                )
                logger.info(f"âœ… {next_page_num}í˜ì´ì§€ë¡œ ì„±ê³µì ìœ¼ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")

                page_num += 1
                time.sleep(2)
                
            except TimeoutException:
                logger.info("ğŸ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                logger.error(f"âŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break

    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ìŠ¤í¬ë© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()

    return job_listings

if __name__ == '__main__':
    url_to_scrape = 'https://www.jobkorea.co.kr/recruit/joblist?menucode=duty'
    scraped_data = scrape_all_job_listings_with_validation(url_to_scrape)
    
    if scraped_data:
        df = pd.DataFrame(scraped_data)
        file_name = "jobkorea_all_listings.csv"
        df.to_csv(file_name, index=False, encoding='utf-8-sig')
        
        logger.info("\n--- í¬ë¡¤ë§ ì™„ë£Œ ---")
        logger.info(f"âœ… ì´ {len(scraped_data)}ê±´ì˜ ì±„ìš© ê³µê³ ë¥¼ '{file_name}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print(df.head())
    else:
        logger.warning("\nâŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")